{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import httpx\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_NOMAD_URL = \"http://nomad-lab.eu/prod/v1/api/v1\"\n",
    "OUTPUT_DIR = \"data/nomad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-27 16:47:40.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtest_nomad_connection\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mTesting connection to NOMAD API...\u001b[0m\n",
      "\u001b[32m2025-10-27 16:47:42.026\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtest_nomad_connection\u001b[0m:\u001b[36m18\u001b[0m - \u001b[32m\u001b[1mConnected to NOMAD API successfully !\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_nomad_connection(base_url: str =\"http://nomad-lab.eu/prod/v1/api/v1\") -> bool:\n",
    "    \"\"\" Test connection to the NOMAD API.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_url : str\n",
    "        The base URL of the NOMAD API. Default is \"http://nomad-lab.eu/prod/v1/api/v1\".\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if the connection is successful, False otherwise.\n",
    "    \"\"\"\n",
    "    logger.info(\"Testing connection to NOMAD API...\")\n",
    "    try:\n",
    "        r = httpx.get(f\"{base_url}/entries\", timeout=5)\n",
    "        if r.status_code == 200:\n",
    "            logger.success(\"Connected to NOMAD API successfully !\")\n",
    "            return True\n",
    "    except httpx.RequestException:\n",
    "        logger.error(\"Failed to connect to NOMAD API.\")\n",
    "        return False\n",
    "\n",
    "test_nomad_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_entries_md_related() -> Tuple[List[Dict[str, Any]], str]:\n",
    "    \"\"\"\n",
    "    Fetch all Molecular Dynamics (MD)-related entries from the NOMAD API.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[Dict[str, Any]], str]:\n",
    "        - A list of entries related to Molecular Dynamics workflows (JSON objects).\n",
    "        Returns an empty list if the request fails.\n",
    "        - The current timestamp in ISO 8601 format (e.g., '2023-03-05T22:01:12').\n",
    "\n",
    "    \"\"\"\n",
    "    logger.info(\"Fetching Molecular Dynamics related entries from NOMAD API...\")\n",
    "    # Current timestamp in ISO format\n",
    "    fetch_time: str = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "    try:\n",
    "        # Build the request URL with a query filtering for 'MolecularDynamics' workflow\n",
    "        url = (\n",
    "            f\"{BASE_NOMAD_URL}/entries/export\"\n",
    "            \"?owner=public\"\n",
    "            \"&json_query=%7B%22results.method.workflow_name%22%3A%22MolecularDynamics%22%7D\"\n",
    "        )\n",
    "\n",
    "        # Perform the HTTP GET request with a long timeout to accommodate large data (usually take less than 3 minutes)\n",
    "        response = httpx.get(url, timeout=1000)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse JSON data\n",
    "        entries_md = response.json()\n",
    "        logger.success(f\"Fetched {len(entries_md)} MD-related entries from NOMAD successfully !\")\n",
    "        return entries_md, fetch_time\n",
    "    \n",
    "    except httpx.HTTPError as e:\n",
    "        logger.error(f\"HTTP error occurred: {e}\")\n",
    "        return [], fetch_time\n",
    "   \n",
    "\n",
    "#nomad_data, fetch_time = fetch_entries_md_related()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"NOMAD\"\n",
    "source_id = f\"https://nomad-lab.eu/prod/v1/gui/search/entries?entry_id={nomad_data[0]['entry_id']}\"\n",
    "doi = nomad_data[0]['references'][1]\n",
    "title = nomad_data[0]['datasets'][0]['dataset_name']\n",
    "date_creation = nomad_data[0]['datasets'][0]['dataset_create_time']\n",
    "date_last_modification = nomad_data[0]['datasets'][0]['dataset_modified_time']\n",
    "nb_files = len(nomad_data[0]['files'])\n",
    "file_names = nomad_data[0]['files']\n",
    "authors = [author_info['name'] for author_info in nomad_data[0]['authors']]\n",
    "license = nomad_data[0]['license']\n",
    "description = nomad_data[0]['comment']\n",
    "file_analysises = nomad_data[0]['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_entry_metadata(data: Dict[str, Any], fetch_time: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse relevant metadata fields from a single NOMAD entry JSON.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : Dict[str, Any]\n",
    "        JSON response for a single NOMAD entry.\n",
    "    fetch_time : str\n",
    "        Timestamp when the data was fetched.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Flattened metadata dictionary for one entry.\n",
    "    \"\"\"\n",
    "    entry_id = data.get(\"entry_id\")\n",
    "    dataset = data.get(\"datasets\", [{}])[0]\n",
    "\n",
    "    return {\n",
    "        \"source\": \"NOMAD\",\n",
    "        \"source_id\": f\"https://nomad-lab.eu/prod/v1/gui/search/entries?entry_id={entry_id}\",\n",
    "        \"doi\": data.get(\"references\"),\n",
    "        \"title\": dataset.get(\"dataset_name\"),\n",
    "        \"date_creation\": dataset.get(\"dataset_create_time\"),\n",
    "        \"date_last_modification\": dataset.get(\"dataset_modified_time\"),\n",
    "        \"date_last_crawled\": fetch_time,\n",
    "        \"nb_files\": len(data.get(\"files\", [])),\n",
    "        \"file_names\": data.get(\"files\", []),\n",
    "        \"authors\": [a.get(\"name\") for a in data.get(\"authors\", [])],\n",
    "        \"license\": data.get(\"license\"),\n",
    "        \"description\": data.get(\"comment\"),\n",
    "        \"file_analyses\": data.get(\"results\"),\n",
    "    }\n",
    "\n",
    "\n",
    "#dict = parse_entry_metadata(nomad_data[0], fetch_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_nomad_dataset_parallel(nomad_data: List[Dict[str, Any]], fetch_time: str, max_workers: int = 8) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse all NOMAD entries in parallel and return a combined DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nomad_data : List[Dict[str, Any]]\n",
    "        List of NOMAD entry JSON objects.\n",
    "    fetch_time : str\n",
    "        Timestamp when data was fetched.\n",
    "    max_workers : int, optional\n",
    "        Maximum number of threads to use for parallel parsing (default is 8).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing parsed metadata for all entries.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(parse_entry_metadata, entry, fetch_time): entry for entry in nomad_data}\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                results.append(future.result())\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing entry: {e}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def save_nomad_metadata(df: pd.DataFrame, output_dir: str = \"data/nomad\", filename: str = \"nomad_metadata.parquet\") -> str:\n",
    "    \"\"\"\n",
    "    Save parsed NOMAD metadata DataFrame to a local file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing parsed NOMAD metadata.\n",
    "    output_dir : str, optional\n",
    "        Directory to store the output file (default is 'data/nomad').\n",
    "    filename : str, optional\n",
    "        Output filename (default is 'nomad_metadata.parquet').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Path to the saved file.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    df.to_parquet(output_path, index=False)\n",
    "    print(f\"✅ NOMAD metadata saved to: {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"NOMAD\"\n",
    "source_id = f\"https://nomad-lab.eu/prod/v1/gui/search/entries?entry_id={nomad_data[0]['entry_id']}\"\n",
    "doi = nomad_data[0]['references'][1]\n",
    "title = nomad_data[0]['datasets'][0]['dataset_name']\n",
    "date_creation = nomad_data[0]['datasets'][0]['dataset_create_time']\n",
    "date_last_modification = nomad_data[0]['datasets'][0]['dataset_modified_time']\n",
    "nb_files = len(nomad_data[0]['files'])\n",
    "file_names = nomad_data[0]['files']\n",
    "authors = [author_info['name'] for author_info in nomad_data[0]['authors']]\n",
    "license = nomad_data[0]['license']\n",
    "description = nomad_data[0]['comment']\n",
    "file_analysises = nomad_data[0]['results']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-27 16:48:02.526\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrap_nomad_data\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mStarting Nomad data scraping...\u001b[0m\n",
      "\u001b[32m2025-10-27 16:48:02.530\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfetch_entries_md_related\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mFetching Molecular Dynamics related entries from NOMAD API...\u001b[0m\n",
      "\u001b[32m2025-10-27 16:50:03.353\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfetch_entries_md_related\u001b[0m:\u001b[36m31\u001b[0m - \u001b[32m\u001b[1mFetched 15934 MD-related entries from NOMAD successfully !\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n",
      "Error parsing entry: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-27 16:50:33.295\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscrap_nomad_data\u001b[0m:\u001b[36m21\u001b[0m - \u001b[32m\u001b[1mScrapped NOMAD data successfully and saved to data/nomad !\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ NOMAD metadata saved to: data/nomad/nomad_metadata.parquet\n"
     ]
    }
   ],
   "source": [
    "def scrap_nomad_data():\n",
    "    \"\"\" Scrap molecular dynamics datasets and files from NOMAD \"\"\"\n",
    "    logger.info(\"Starting Nomad data scraping...\")\n",
    "\n",
    "    if test_nomad_connection:\n",
    "        # Define output directory\n",
    "        output_dir = os.path.join(\"data\", \"nomad\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Fetch NOMAD entries metadata\n",
    "        nomad_data, fetch_time = fetch_entries_md_related()\n",
    "        if nomad_data == []:\n",
    "            logger.warning(\"No data fetched from NOMAD.\")\n",
    "            return\n",
    "        # Parse NOMAD entries metadata in parallel\n",
    "        nomad_metadata_df = parse_nomad_dataset_parallel(nomad_data, fetch_time)\n",
    "\n",
    "        # Save parsed metadata to local file\n",
    "        save_nomad_metadata(nomad_metadata_df, output_dir=output_dir)\n",
    "    \n",
    "        logger.success(f\"Scrapped NOMAD data successfully and saved to {output_dir} !\")\n",
    "\n",
    "    else:\n",
    "        logger.error(\"Cannot scrap data, no connection to NOMAD API.\")\n",
    "        sys.exit()\n",
    "\n",
    "scrap_nomad_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "922ecd667b350a70e1a7163cd8cc1b4473910332ceb383a13d756425e7d9fb94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
